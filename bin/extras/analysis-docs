Definitions for track-analysis objects returned by spotify api
==============================================================

track data
----------

  - time signature:
    ---------------
      an estimated overall time signature of a track. The time signature (meter) is a notational convention to specify how many beats are in each bar (or measure).

  - key:
    ----
      the estimated overall key of a track. The key identifies the tonic triad, the chord, major or minor, which represents the final point of rest of a piece.

  - mode:
    -----
      indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived.

  - tempo:
    ------
      the overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.

  - loudness:
    ---------
      the overall loudness of a track in decibels (dB). Loudness values in the Analyzer are averaged across an entire track and are useful for comparing relative loudness of segments and tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude).

  - duration:
    ---------
      the duration of a track in seconds as precisely computed by the audio decoder.

  - end of fade in:
    ---------------
      the end of the fade-in introduction to a track in seconds.

  - start of fade out:
    ------------------
      the start of the fade out at the end of a track in seconds.

  - codestring, echoprintstring:
    ----------------------------
      these represent two different audio fingerprints computed on the audio and are used by other Echo Nest services for song identification. For more information on Echoprint, see http://echoprint.me.

  - synchstring:
    ------------
      a synchronization code that allows a client player to synchronize the analysis data to the audio waveform with sample accuracy, regardless of its decoder type or version. See Synchstring section.

  - rhythmstring:
    -------------
      a representation of spectro-temporal transients as binary events. This temporal data distributed on 8 frequency channels aims to be independent of timbre and pitch representations. See Rhythmstring section.

- timbre, pitch, and loudness are described in detail as part of the segments interpretation below.

--------------
sequenced data
--------------
  - the Analyzer breaks down the audio into musically relevant elements that occur sequenced in time.

  From smallest to largest those include:

  - segments:
    ---------
      a set of sound entities (typically under a second) each relatively uniform in timbre and harmony. Segments are characterized by their perceptual onsets and duration in seconds, loudness (dB), pitch and timbral content.

      - loudness_start:
        ---------------
          indicates the loudness level at the start of the segment

      - loudness_max_time:
        ------------------
          offset within the segment of the point of maximum loudness

      - loudness_max:
        -------------
          peak loudness value within the segment

  - tatums:
    -------
      list of tatum markers, in seconds. Tatums represent the lowest regular pulse train that a listener intuitively infers from the timing of perceived musical events (segments).

  - beats:
    ------
      list of beat markers, in seconds. A beat is the basic time unit of a piece of music; for example, each tick of a metronome. Beats are typically multiples of tatums.

  - bars:
    -----
      list of bar markers, in seconds. A bar (or measure) is a segment of time defined as a given number of beats. Bar offsets also indicate downbeats, the first beat of the measure.

  - sections:
    ---------
      a set of section markers, in seconds. Sections are defined by large variations in rhythm or timbre, e.g. chorus, verse, bridge, guitar solo, etc. Each section contains its own descriptions of tempo, key, mode, time_signature, and loudness.

-----------------------------------
Additional notes from documentation
-----------------------------------

Rhythm
------
  Beats are subdivisions of bars. Tatums are subdivisions of beats. That is, bars always align with a beat and ditto tatums.

  Note that a low confidence does not necessarily mean the value is inaccurate. Exceptionally, a confidence of -1 indicates “no” value: the corresponding element must be discarded. A track may result with no bar, no beat, and/or no tatum if no periodicity was detected. The time signature ranges from 3 to 7 indicating time signatures of 3/4, to 7/4. A value of -1 may indicate no time signature, while a value of 1 indicates a rather complex or changing time signature.

Pitch
-----
  The key is a track-level attribute ranging from 0 to 11 and corresponding to one of the 12 keys: C, C#, D, etc. up to B. If no key was detected, the value is -1. The mode is equal to 0 or 1 for “minor” or “major” and may be -1 in case of no result. Note that the major key (e.g. C major) could more likely be confused with the minor key at 3 semitones lower (e.g. A minor) as both keys carry the same pitches. Harmonic details are given in segments below.

Segments
--------
  Beyond timing information (start, duration), segments include loudness, pitch, and timbre features.

    loudness information (i.e. attack, decay) is given by three data points, including dB value at onset (loudness_start), dB value at peak (loudness_max), and segment-relative offset for the peak loudness (loudness_max_time). The dB value at onset is equivalent to the dB value at offset for the preceding segment. The last segment specifies a dB value at offset (loudness_end) as well.

    pitch content is given by a “chroma” vector, corresponding to the 12 pitch classes C, C#, D to B, with values ranging from 0 to 1 that describe the relative dominance of every pitch in the chromatic scale. For example a C Major chord would likely be represented by large values of C, E and G (i.e. classes 0, 4, and 7). Vectors are normalized to 1 by their strongest dimension, therefore noisy sounds are likely represented by values that are all close to 1, while pure tones are described by one value at 1 (the pitch) and others near 0.

    timbre is the quality of a musical note or sound that distinguishes different types of musical instruments, or voices. It is a complex notion also referred to as sound color, texture, or tone quality, and is derived from the shape of a segment’s spectro-temporal surface, independently of pitch and loudness. The Echo Nest Analyzer’s timbre feature is a vector that includes 12 unbounded values roughly centered around 0. Those values are high level abstractions of the spectral surface, ordered by degree of importance. For completeness however, the first dimension represents the average loudness of the segment; second emphasizes brightness; third is more closely correlated to the flatness of a sound; fourth to sounds with a stronger attack; etc. See an image below representing the 12 basis functions (i.e. template segments). The actual timbre of the segment is best described as a linear combination of these 12 basis functions weighted by the coefficient values: timbre = c1 x b1 + c2 x b2 + ... + c12 x b12, where c1 to c12 represent the 12 coefficients and b1 to b12 the 12 basis functions as displayed below. Timbre vectors are best used in comparison with each other.

Confidence Values
-----------------
  Many elements at the track and lower levels of analysis include confidence values, a floating-point number ranging from 0.0 to 1.0. Confidence indicates the reliability of its corresponding attribute. Elements carrying a small confidence value should be considered speculative. There may not be sufficient data in the audio to compute the element with high certainty.
